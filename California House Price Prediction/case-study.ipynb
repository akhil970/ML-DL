{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt #visualize data\n",
    "import seaborn as sns # visualize data with more appealing \n",
    "import scipy.stats as stats # \n",
    "import statsmodels.api as sm\n",
    "from sklearn.linear_model import LinearRegression #machine learning\n",
    "from sklearn.model_selection import train_test_split "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading California Data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = \"housing.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(file_path)\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[\"ocean_proximity\"].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Missing Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for missing values\n",
    "missing_values = data.isnull().sum()\n",
    "\n",
    "# calculate the missing data percentage in each column\n",
    "missing_percentage = (missing_values / len(data)) * 100\n",
    "\n",
    "# Display the missing data statistics\n",
    "print(\"Missing values in each column:\\n\",missing_values)\n",
    "print(\"Percentage of missing data:\\n\",missing_percentage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove rows with missing values\n",
    "data_cleaned = data.dropna()\n",
    "\n",
    "print(\"Missing valuesin each column after cleaning:\")\n",
    "print(data_cleaned.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "sns.set_theme(style=\"whitegrid\")\n",
    "plt.figure(figsize=(10,6))\n",
    "sns.histplot(data_cleaned['median_house_value'],color='forestgreen',kde=True)\n",
    "plt.title('Distribution of Median House Value')\n",
    "plt.xlabel('Median house value')\n",
    "plt.ylabel('Frequency')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using InterQuantile Range to Remove Outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Q1 = data_cleaned['median_house_value'].quantile(0.25)\n",
    "Q3 = data_cleaned['median_house_value'].quantile(0.75)\n",
    "IQR = Q3-Q1\n",
    "\n",
    "#Define the bounds for the outliers\n",
    "lower_bound = Q1 - 1.5 * IQR\n",
    "upper_bound = Q3 + 1.5 * IQR\n",
    "\n",
    "#Remove outliers\n",
    "data_no_outliers_1 = data_cleaned[(data_cleaned['median_house_value'] >= lower_bound) & (data_cleaned['median_house_value'] <= upper_bound)]\n",
    "\n",
    "#check the shpae of the data before and after removal of outliers\n",
    "print(\"Original data shape\",data_cleaned.shape)\n",
    "print(\"New data shape without outliers\",data_no_outliers_1.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BoxPlot for outlier detection\n",
    "\n",
    "Outliers in Median Income\n",
    "        |\n",
    "        |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,6))\n",
    "sns.boxplot(x=data_no_outliers_1['median_income'],color='beige')\n",
    "plt.title('Outlier Analysis in Median Income')\n",
    "plt.xlabel('Median Income')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Q1 = data_no_outliers_1['median_income'].quantile(0.25)\n",
    "Q3 = data_no_outliers_1['median_income'].quantile(0.75)\n",
    "IQR = Q3-Q1\n",
    "\n",
    "#Define the bounds for the outliers\n",
    "lower_bound = Q1 - 1.5 * IQR\n",
    "upper_bound = Q3 + 1.5 * IQR\n",
    "\n",
    "#Remove outliers\n",
    "data_no_outliers_2 = data_no_outliers_1[(data_no_outliers_1['median_income']>= lower_bound) & (data_no_outliers_1['median_income'] <= upper_bound)]\n",
    "\n",
    "#check the shpae of the data before and after removal of outliers\n",
    "print(\"Original data shape\",data_no_outliers_1.shape)\n",
    "print(\"New data shape without outliers\",data_no_outliers_2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data_no_outliers_2\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Correlation Heatmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,8))\n",
    "#data.select_dtypes(include=['number']) specifies to select only numerical to avoid conflict with string datatypes while plotting heatmaps\n",
    "sns.heatmap(data.select_dtypes(include=['number']).corr(),annot=True,cmap='Greens')\n",
    "plt.title('correlation Heatmap of Housing Data')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the total bedrooms column is being dropped below becuase it is causing high dependency with two independent variable which will cause a deviation while training the models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.drop(\"total_bedrooms\",axis=1)\n",
    "data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unique value count for categorical data\n",
    "for column in ['ocean_proximity']:\n",
    "    print(f\"Unique values in {column}:\",data[column].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "String Data Categorization to Dummy variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "ocean_proximity_dummies = pd.get_dummies(data['ocean_proximity'], prefix='ocean_proximity').astype(int)\n",
    "data = pd.concat([data.drop(\"ocean_proximity\", axis=1), ocean_proximity_dummies],axis =1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(ocean_proximity_dummies.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ocean_proximity_dummies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split and Train Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.drop(\"ocean_proximity_ISLAND\",axis=1)\n",
    "data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head(1)\n",
    "data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define features (Independent Variables) and target (dependent variables)\n",
    "features = ['longitude', 'latitude', 'housing_median_age', 'total_rooms',\n",
    "       'population', 'households', 'median_income', 'median_house_value',\n",
    "       'ocean_proximity_<1H OCEAN', 'ocean_proximity_INLAND',\n",
    "       'ocean_proximity_NEAR BAY', 'ocean_proximity_NEAR OCEAN']\n",
    "target = [\"median_house_value\"]\n",
    "\n",
    "X = data[features]\n",
    "y = data[target]\n",
    "\n",
    "# Split the data in to test and trianing\n",
    "# Test size specifies what portion of data should use for training\n",
    "# random_state ensures reproducibility of your split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1111)\n",
    "\n",
    "# Check the size of the splits\n",
    "print(f'Training set size: {X_train.shape[0]} samples')\n",
    "print(f'Test set size: {X_test.shape[0]} samples')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding a constant to the predictors because statsmodels OLS doesn't include it by default\n",
    "X_train_constant = sm.add_constant(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the OLS model\n",
    "model_fitted = sm.OLS(y_train,X_train_constant).fit()\n",
    "\n",
    "# Printing Summary\n",
    "print(model_fitted.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prediction/Testing  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding a constant to the test predictors\n",
    "X_test_const = sm.add_constant(X_test)\n",
    "\n",
    "# Making predictions on the test set\n",
    "test_predictions = model_fitted.predict(X_test_const)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Checking OLS Assumptions\n",
    "\n",
    "Assumption 1: Linearity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scatter plot for observed vs predicted values on test data\n",
    "plt.scatter(y_test, test_predictions, color='forestgreen')\n",
    "plt.xlabel('Observed Values')\n",
    "plt.ylabel('Predicted Values')\n",
    "plt.title('Observed vs Predicted Values on Test Data')\n",
    "plt.plot(y_test, y_test, color='black') # line for perfect prediction (true values)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assumption 2: Random Sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the mean of the residuals\n",
    "mean_residuals = np.mean(model_fitted.resid)\n",
    "\n",
    "print(f\"The mean of the residuals is {np.round(mean_residuals,2)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting residuals\n",
    "plt.scatter(model_fitted.fittedvalues, model_fitted.resid, color='forestgreen')\n",
    "plt.axhline(y=0, color='red', linestyle='--')\n",
    "plt.xlabel('Fitted Values')\n",
    "plt.ylabel('Residuals')\n",
    "plt.title('Residuals vs Fitted Values')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assumption 3: Exogeneity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the residuals\n",
    "residuals = model_fitted.resid\n",
    "\n",
    "# Check for correlation between residuals and each predictor\n",
    "for column in X_train.columns:\n",
    "    corr_coefficient = np.corrcoef(X_train[column], residuals)[0,1]\n",
    "    print(f'Correlation between residuals and {column}: {np.round(corr_coefficient,2)}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Asumption 4: Homoskedasticity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting residuals\n",
    "plt.scatter(model_fitted.fittedvalues, model_fitted.resid, color='forestgreen')\n",
    "plt.axhline(y=0, color='red', linestyle='--')\n",
    "plt.xlabel('Fitted Values')\n",
    "plt.ylabel('Residuals')\n",
    "plt.title('Residuals vs Fitted Values')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train/Test/Evaluation with Sklearn\n",
    "\n",
    "\n",
    "Scaling the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "# Initialize the StandardScaler\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Fit the scaler on the training data and transform it\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "\n",
    "# Apply the same transformation to the test data\n",
    "X_test_scaled = scaler.transform(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from math import sqrt\n",
    "\n",
    "# Create and fit the model\n",
    "\n",
    "lr = LinearRegression()\n",
    "lr.fit(X_train_scaled, y_train)\n",
    "print()\n",
    "\n",
    "# Make the predictions on the scaled test data\n",
    "y_pred = lr.predict(X_test_scaled)\n",
    "\n",
    "#Calculate MSE and RMSE\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "rmse = sqrt(mse)\n",
    "\n",
    "# Output the performance metrics\n",
    "print(f'MSE on Test Set: {mse}')\n",
    "print(f'RMSE on Test Set: {rmse}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(y_test,y_pred)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import r2_score\n",
    "\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "print(r2)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
